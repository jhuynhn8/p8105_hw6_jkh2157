---
title: "p8105_hw6_jkh2157"
output: github_document
date: "2025-12-01"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
library(modelr)
library(purrr)
library(readr)
library(p8105.datasets)
```

# Problem 1 

#### Data Cleaning 

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv") |>
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    victim_race = fct_relevel(victim_race, "White")
  ) |>
  filter(
    !(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")),
    victim_race %in% c("White", "Black")
  ) |>
  mutate(resolved = as.numeric(disposition == "Closed by arrest"))

#checking to make sure that it correctly dropped those states 

homicide_df |> 
  distinct(city_state) |> 
  filter(city_state %in% c(
    "Dallas, TX",
    "Phoenix, AZ",
    "Kansas City, MO",
    "Tulsa, AL"
  ))
```

#### Logistic Regression for Baltimore 

```{r}
baltimore_df =
  homicide_df |> 
  filter(city_state == "Baltimore, MD")

fit_baltimore =
  baltimore_df |>
  glm(resolved ~ victim_age + victim_race + victim_sex,
      data = _,
      family = binomial())

baltimore_or =
  fit_baltimore |>
  tidy() |>
  mutate(OR = exp(estimate)) |>
  select(term, log_OR = estimate, OR, p.value)

baltimore_or |> 
  filter(term == "victim_sexMale")

#creating a nicer table 

baltimore_results =
  fit_baltimore |> 
  tidy() |>
  mutate(
    OR = exp(estimate),
    ci_lower = exp(estimate - 1.96 * std.error),
    ci_upper = exp(estimate + 1.96 * std.error)
  ) |>
  filter(term == "victim_sexMale") |>
  select(
    term,
    log_OR = estimate,
    OR,
    ci_lower,
    ci_upper,
    p.value
  )

baltimore_results |> 
  knitr::kable(digits = 3)

```

**Interpretation**: Holding age and race constant, male victims have significantly lower odds of having their homicide solved compared to female victims. The adjusted odds ratio is 0.43, indicating that the odds of case resolution for male victims are about 57% lower than for female victims. The association is also statistically significant (p<0.0001) 

#### Linear Regression for other cities: Using nest and map 

```{r}
city_results =
  homicide_df |> 
  nest(data = -city_state) |>
  mutate(
    models = map(data, \(df)
      glm(resolved ~ victim_age + victim_race + victim_sex,
          data = df,
          family = binomial())
    ),
    results = map(models, tidy)
  ) |>
  select(-data, -models) |>
  unnest(results) |>
  mutate(OR = exp(estimate)) |>
  filter(term == "victim_sexMale") |>
  group_by(city_state) |>
  mutate(
    ci_lower = exp(estimate - 1.96 * std.error),
    ci_upper = exp(estimate + 1.96 * std.error)
  ) |>
  ungroup()

#creating nicer tables 
city_results_table =
  city_results |> 
  select(
    city_state,
    log_OR = estimate,
    OR,
    ci_lower,
    ci_upper,
    p.value
  ) |>
  arrange(OR)

city_results_table |> 
  knitr::kable(digits = 3)

model.matrix(~ victim_sex, data = homicide_df) |> head()
```

**Interpretation**: Across U.S. cities, the adjusted odds of solving a homicide are consistently lower for male victims than for female victims, even after controlling for age and race.

#### Plot estimated ORs and CI 

```{r}
city_results |> 
  ggplot(aes(x = fct_reorder(city_state, OR), y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper)) +
  coord_flip() +
  labs(
    x = "City",
    y = "Adjusted Odds Ratio",
    title = "Adjusted ORs for Homicide Resolution Across U.S. Cities"
  )
```

# Problem 2 

#### Looking at data set 

```{r}
data("weather_df")

weather_df =
  weather_df |> 
  drop_na(tmax, tmin, prcp)
```

#### Linear Model and Bootstrap samples 

```{r}
fit = lm(tmax ~ tmin + prcp, data = weather_df)

#checking 
glance(fit)
tidy(fit)

#drawing samples 
set.seed(1)
boot_straps =
  weather_df |> 
  modelr::bootstrap(n = 5000)

#compute 
boot_results =
  boot_straps |> 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df)),
    glance = map(models, broom::glance),
    tidy   = map(models, broom::tidy)
  ) |>
  mutate(
    r2 = map_dbl(glance, "r.squared"),
    beta_ratio = map_dbl(tidy, \(x) {
      b1 = x |> filter(term == "tmin") |> pull(estimate)
      b2 = x |> filter(term == "prcp") |> pull(estimate)
      b1 / b2
    })
  ) |>
  select(r2, beta_ratio)

#Creating clean table with all of these estimates 
orig_table =
  tibble(
    quantity = c("R-squared", "beta_tmin", "beta_prcp", "beta_ratio"),
    estimate = c(
      glance(fit)$r.squared,
      tidy(fit) |> filter(term == "tmin") |> pull(estimate),
      tidy(fit) |> filter(term == "prcp") |> pull(estimate),
      (tidy(fit) |> filter(term == "tmin") |> pull(estimate)) /
        (tidy(fit) |> filter(term == "prcp") |> pull(estimate))
    )
  )

orig_table |> knitr::kable(digits = 3)
```

### Plotting 

#### R squared Distribution 

```{r}
boot_results |> 
  ggplot(aes(x = r2)) +
  geom_density(fill = "skyblue", alpha = 0.5) +
  labs(
    title = "Bootstrap Distribution of R²",
    x = "R²",
    y = "Density"
  )
```

#### Beta Distribution 

```{r}
boot_results |> 
  ggplot(aes(x = beta_ratio)) +
  geom_density(fill = "coral", alpha = 0.5) +
  labs(
    title = "Bootstrap Distribution of β 1 / β 2",
    x = "β₁ / β₂",
    y = "Density"
  )
```

**Interpretation for the bootstrap distribution of R Squared**: The bootstrap distribution of R squared is very narrow (0.93-0.95) and tightly centered around ~0.94, indicating that the linear model consistently explains about 94% of the variation in maximum temperature across resampled datasets. The small spread of the distribution shows that the model’s explanatory power is highly stable since the repeated sampling produces nearly identical values. This suggests that the relationship between tmax and its predictors is strong and not that sensitive to sampling variability.

**Interpretation for the bootstrap distribution of Betas**: The distribution of the coefficient ratio is much wider and skewed compared to R squared. This shows that there is greater sampling variability in the ratio. Looking at the estimates, the prcp coefficient is near 0 which likely explains why small estimation differences across resamples is magnified. Therefore, the coefficients are much more sensitive and not as stable. 
